{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlGPTRmTknIY",
        "outputId": "35b5f328-fcfd-4555-fbd5-5caba3e8efe2"
      },
      "outputs": [],
      "source": [
        "# !python3 -V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "1svKyYpL4_Je",
        "outputId": "49529ee0-f3fc-48f0-f684-7a2b5cbca42e"
      },
      "outputs": [],
      "source": [
        "# !pip install -U git+https://github.com/amitness/colab-connect.git\n",
        "# from colabconnect import colabconnect\n",
        "\n",
        "# colabconnect()\n",
        "# mount_gdrive = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fZe1wOxAqxI"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcSFok0OAsjs"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CvNsfOw3ABbK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install transformer_lens\n",
        "# !pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6uagRYLkCC6_"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "import torch\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pprint\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import HfApi\n",
        "from IPython.display import HTML\n",
        "from functools import partial\n",
        "import tqdm.notebook as tqdm\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXl91Y1dAtyT"
      },
      "source": [
        "## Defining the Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qCF9odNdAvKX"
      },
      "outputs": [],
      "source": [
        "cfg = {\n",
        "    \"seed\": 49,\n",
        "    \"batch_size\": 4096,\n",
        "    \"buffer_mult\": 384,\n",
        "    \"lr\": 1e-4,\n",
        "    \"num_tokens\": int(2e9),\n",
        "    \"l1_coeff\": 3e-4,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.99,\n",
        "    \"dict_mult\": 8,\n",
        "    \"seq_len\": 128,\n",
        "    \"d_mlp\": 2048,\n",
        "    \"enc_dtype\":\"fp32\",\n",
        "    \"remove_rare_dir\": False,\n",
        "}\n",
        "cfg[\"model_batch_size\"] = 64\n",
        "cfg[\"buffer_size\"] = cfg[\"batch_size\"] * cfg[\"buffer_mult\"]\n",
        "cfg[\"buffer_batches\"] = cfg[\"buffer_size\"] // cfg[\"seq_len\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kn1E_44gCa-Z"
      },
      "outputs": [],
      "source": [
        "DTYPES = {\"fp32\": torch.float32, \"fp16\": torch.float16, \"bf16\": torch.bfloat16}\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        d_hidden = cfg[\"d_mlp\"] * cfg[\"dict_mult\"]\n",
        "        d_mlp = cfg[\"d_mlp\"]\n",
        "        l1_coeff = cfg[\"l1_coeff\"]\n",
        "        dtype = DTYPES[cfg[\"enc_dtype\"]]\n",
        "        torch.manual_seed(cfg[\"seed\"])\n",
        "        self.W_enc = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_mlp, d_hidden, dtype=dtype)))\n",
        "        self.W_dec = nn.Parameter(torch.nn.init.kaiming_uniform_(torch.empty(d_hidden, d_mlp, dtype=dtype)))\n",
        "        self.b_enc = nn.Parameter(torch.zeros(d_hidden, dtype=dtype))\n",
        "        self.b_dec = nn.Parameter(torch.zeros(d_mlp, dtype=dtype))\n",
        "\n",
        "        self.W_dec.data[:] = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        self.d_hidden = d_hidden\n",
        "        self.l1_coeff = l1_coeff\n",
        "\n",
        "        self.to(\"cuda\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_cent = x - self.b_dec\n",
        "        acts = F.relu(x_cent @ self.W_enc + self.b_enc)\n",
        "        x_reconstruct = acts @ self.W_dec + self.b_dec\n",
        "        l2_loss = (x_reconstruct.float() - x.float()).pow(2).sum(-1).mean(0)\n",
        "        l1_loss = self.l1_coeff * (acts.float().abs().sum())\n",
        "        loss = l2_loss + l1_loss\n",
        "        return loss, x_reconstruct, acts, l2_loss, l1_loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def remove_parallel_component_of_grads(self):\n",
        "        W_dec_normed = self.W_dec / self.W_dec.norm(dim=-1, keepdim=True)\n",
        "        W_dec_grad_proj = (self.W_dec.grad * W_dec_normed).sum(-1, keepdim=True) * W_dec_normed\n",
        "        self.W_dec.grad -= W_dec_grad_proj\n",
        "\n",
        "    # def get_version(self):\n",
        "    #     return 1+max([int(file.name.split(\".\")[0]) for file in list(SAVE_DIR.iterdir()) if \"pt\" in str(file)])\n",
        "\n",
        "    # def save(self):\n",
        "    #     version = self.get_version()\n",
        "    #     torch.save(self.state_dict(), SAVE_DIR/(str(version)+\".pt\"))\n",
        "    #     with open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"w\") as f:\n",
        "    #         json.dump(cfg, f)\n",
        "    #     print(\"Saved as version\", version)\n",
        "\n",
        "    # def load(cls, version):\n",
        "    #     cfg = (json.load(open(SAVE_DIR/(str(version)+\"_cfg.json\"), \"r\")))\n",
        "    #     pprint.pprint(cfg)\n",
        "    #     self = cls(cfg=cfg)\n",
        "    #     self.load_state_dict(torch.load(SAVE_DIR/(str(version)+\".pt\")))\n",
        "    #     return self\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_hf(cls, version):\n",
        "        \"\"\"\n",
        "        Loads the saved autoencoder from HuggingFace.\n",
        "\n",
        "        Version is expected to be an int, or \"run1\" or \"run2\"\n",
        "\n",
        "        version 25 is the final checkpoint of the first autoencoder run,\n",
        "        version 47 is the final checkpoint of the second autoencoder run.\n",
        "        \"\"\"\n",
        "        if version==\"run1\":\n",
        "            version = 25\n",
        "        elif version==\"run2\":\n",
        "            version = 47\n",
        "\n",
        "        cfg = utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}_cfg.json\")\n",
        "        pprint.pprint(cfg)\n",
        "        self = cls(cfg=cfg)\n",
        "        self.load_state_dict(utils.download_file_from_hf(\"NeelNanda/sparse_autoencoder\", f\"{version}.pt\", force_is_torch=True))\n",
        "        return self\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysejlJ4ZLrl7"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjpJCFxGLtoK"
      },
      "source": [
        "### Get Reconstruction Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pXH4pos0LvAs"
      },
      "outputs": [],
      "source": [
        "def replacement_hook(mlp_post, hook, encoder):\n",
        "    mlp_post_reconstr = encoder(mlp_post)[1]\n",
        "    return mlp_post_reconstr\n",
        "\n",
        "def mean_ablate_hook(mlp_post, hook):\n",
        "    mlp_post[:] = mlp_post.mean([0, 1])\n",
        "    return mlp_post\n",
        "\n",
        "def zero_ablate_hook(mlp_post, hook):\n",
        "    mlp_post[:] = 0.\n",
        "    return mlp_post\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_recons_loss(num_batches=5, local_encoder=None):\n",
        "    if local_encoder is None:\n",
        "        local_encoder = encoder\n",
        "    loss_list = []\n",
        "    for i in range(num_batches):\n",
        "        tokens = all_tokens[torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
        "        loss = model(tokens, return_type=\"loss\")\n",
        "        recons_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), partial(replacement_hook, encoder=local_encoder))])\n",
        "        # mean_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), mean_ablate_hook)])\n",
        "        zero_abl_loss = model.run_with_hooks(tokens, return_type=\"loss\", fwd_hooks=[(utils.get_act_name(\"post\", 0), zero_ablate_hook)])\n",
        "        loss_list.append((loss, recons_loss, zero_abl_loss))\n",
        "    losses = torch.tensor(loss_list)\n",
        "    loss, recons_loss, zero_abl_loss = losses.mean(0).tolist()\n",
        "\n",
        "    print(f\"loss: {loss:.4f}, recons_loss: {recons_loss:.4f}, zero_abl_loss: {zero_abl_loss:.4f}\")\n",
        "    score = ((zero_abl_loss - recons_loss)/(zero_abl_loss - loss))\n",
        "    print(f\"Reconstruction Score: {score:.2%}\")\n",
        "    # print(f\"{((zero_abl_loss - mean_abl_loss)/(zero_abl_loss - loss)).item():.2%}\")\n",
        "    return score, loss, recons_loss, zero_abl_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vz9UyVGBdWX"
      },
      "source": [
        "### Get Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NeslkII9Bqa3"
      },
      "outputs": [],
      "source": [
        "# Frequency\n",
        "@torch.no_grad()\n",
        "def get_freqs(num_batches=25, local_encoder=None):\n",
        "    if local_encoder is None:\n",
        "        local_encoder = encoder\n",
        "    act_freq_scores = torch.zeros(local_encoder.d_hidden, dtype=torch.float32).cuda()\n",
        "    total = 0\n",
        "    for i in tqdm.trange(num_batches):\n",
        "        tokens = all_tokens[torch.randperm(len(all_tokens))[:cfg[\"model_batch_size\"]]]\n",
        "\n",
        "        _, cache = model.run_with_cache(tokens, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
        "        mlp_acts = cache[utils.get_act_name(\"post\", 0)]\n",
        "        mlp_acts = mlp_acts.reshape(-1, d_mlp)\n",
        "\n",
        "        hidden = local_encoder(mlp_acts)[2]\n",
        "\n",
        "        act_freq_scores += (hidden > 0).sum(0)\n",
        "        total+=hidden.shape[0]\n",
        "    act_freq_scores /= total\n",
        "    num_dead = (act_freq_scores==0).float().mean()\n",
        "    print(\"Num dead\", num_dead)\n",
        "    return act_freq_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZiWeLhaBrFK"
      },
      "source": [
        "## Visualise Feature Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zkOsX7DvBs0u"
      },
      "outputs": [],
      "source": [
        "from html import escape\n",
        "import colorsys\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "SPACE = \"·\"\n",
        "NEWLINE=\"↩\"\n",
        "TAB = \"→\"\n",
        "\n",
        "def create_html(strings, values, max_value=None, saturation=0.5, allow_different_length=False, return_string=False):\n",
        "    # escape strings to deal with tabs, newlines, etc.\n",
        "    escaped_strings = [escape(s, quote=True) for s in strings]\n",
        "    processed_strings = [\n",
        "        s.replace(\"\\n\", f\"{NEWLINE}<br/>\").replace(\"\\t\", f\"{TAB}&emsp;\").replace(\" \", \"&nbsp;\")\n",
        "        for s in escaped_strings\n",
        "    ]\n",
        "\n",
        "    if isinstance(values, torch.Tensor) and len(values.shape)>1:\n",
        "        values = values.flatten().tolist()\n",
        "\n",
        "    if not allow_different_length:\n",
        "        assert len(processed_strings) == len(values)\n",
        "\n",
        "    # scale values\n",
        "    if max_value is None:\n",
        "        max_value = max(max(values), -min(values))+1e-3\n",
        "    scaled_values = [v / max_value * saturation for v in values]\n",
        "\n",
        "    # create html\n",
        "    html = \"\"\n",
        "    for i, s in enumerate(processed_strings):\n",
        "        if i<len(scaled_values):\n",
        "            v = scaled_values[i]\n",
        "        else:\n",
        "            v = 0\n",
        "        if v < 0:\n",
        "            hue = 0  # hue for red in HSV\n",
        "        else:\n",
        "            hue = 0.66  # hue for blue in HSV\n",
        "        rgb_color = colorsys.hsv_to_rgb(\n",
        "            hue, v, 1\n",
        "        )  # hsv color with hue 0.66 (blue), saturation as v, value 1\n",
        "        hex_color = \"#%02x%02x%02x\" % (\n",
        "            int(rgb_color[0] * 255),\n",
        "            int(rgb_color[1] * 255),\n",
        "            int(rgb_color[2] * 255),\n",
        "        )\n",
        "        html += f'<span style=\"background-color: {hex_color}; border: 1px solid lightgray; font-size: 16px; border-radius: 3px;\">{s}</span>'\n",
        "    if return_string:\n",
        "        return html\n",
        "    else:\n",
        "        display(HTML(html))\n",
        "\n",
        "def basic_feature_vis(text, feature_index, max_val=0):\n",
        "    feature_in = encoder.W_enc[:, feature_index]\n",
        "    feature_bias = encoder.b_enc[feature_index]\n",
        "    _, cache = model.run_with_cache(text, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
        "    mlp_acts = cache[utils.get_act_name(\"post\", 0)][0]\n",
        "    feature_acts = F.relu((mlp_acts - encoder.b_dec) @ feature_in + feature_bias)\n",
        "    if max_val==0:\n",
        "        max_val = max(1e-7, feature_acts.max().item())\n",
        "        # print(max_val)\n",
        "    # if min_val==0:\n",
        "    #     min_val = min(-1e-7, feature_acts.min().item())\n",
        "    return basic_token_vis_make_str(text, feature_acts, max_val)\n",
        "def basic_token_vis_make_str(strings, values, max_val=None):\n",
        "    if not isinstance(strings, list):\n",
        "        strings = model.to_str_tokens(strings)\n",
        "    values = utils.to_numpy(values)\n",
        "    if max_val is None:\n",
        "        max_val = values.max()\n",
        "    # if min_val is None:\n",
        "    #     min_val = values.min()\n",
        "    header_string = f\"<h4>Max Range <b>{values.max():.4f}</b> Min Range: <b>{values.min():.4f}</b></h4>\"\n",
        "    header_string += f\"<h4>Set Max Range <b>{max_val:.4f}</b></h4>\"\n",
        "    # values[values>0] = values[values>0]/ma|x_val\n",
        "    # values[values<0] = values[values<0]/abs(min_val)\n",
        "    body_string = create_html(strings, values, max_value=max_val, return_string=True)\n",
        "    return header_string + body_string\n",
        "# display(HTML(basic_token_vis_make_str(tokens[0, :10], mlp_acts[0, :10, 7], 0.1)))\n",
        "# # %%\n",
        "# The `with gr.Blocks() as demo:` syntax just creates a variable called demo containing all these components\n",
        "import gradio as gr\n",
        "try:\n",
        "    demos[0].close()\n",
        "except:\n",
        "    pass\n",
        "demos = [None]\n",
        "def make_feature_vis_gradio(feature_id, starting_text=None, batch=None, pos=None):\n",
        "    if starting_text is None:\n",
        "        starting_text = model.to_string(all_tokens[batch, 1:pos+1])\n",
        "    try:\n",
        "        demos[0].close()\n",
        "    except:\n",
        "        pass\n",
        "    with gr.Blocks() as demo:\n",
        "        gr.HTML(value=f\"Hacky Interactive Neuroscope for gelu-1l\")\n",
        "        # The input elements\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                text = gr.Textbox(label=\"Text\", value=starting_text)\n",
        "                # Precision=0 makes it an int, otherwise it's a float\n",
        "                # Value sets the initial default value\n",
        "                feature_index = gr.Number(\n",
        "                    label=\"Feature Index\", value=feature_id, precision=0\n",
        "                )\n",
        "                # # If empty, these two map to None\n",
        "                max_val = gr.Number(label=\"Max Value\", value=None)\n",
        "                # min_val = gr.Number(label=\"Min Value\", value=None)\n",
        "                inputs = [text, feature_index, max_val]\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # The output element\n",
        "                out = gr.HTML(label=\"Neuron Acts\", value=basic_feature_vis(starting_text, feature_id))\n",
        "        for inp in inputs:\n",
        "            inp.change(basic_feature_vis, inputs, out)\n",
        "    demo.launch(share=True)\n",
        "    demos[0] = demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgSWkew4SYWz"
      },
      "source": [
        "### Inspecting Top Logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zzY9rOh-SaUm"
      },
      "outputs": [],
      "source": [
        "SPACE = \"·\"\n",
        "NEWLINE=\"↩\"\n",
        "TAB = \"→\"\n",
        "def process_token(s):\n",
        "    if isinstance(s, torch.Tensor):\n",
        "        s = s.item()\n",
        "    if isinstance(s, np.int64):\n",
        "        s = s.item()\n",
        "    if isinstance(s, int):\n",
        "        s = model.to_string(s)\n",
        "    s = s.replace(\" \", SPACE)\n",
        "    s = s.replace(\"\\n\", NEWLINE+\"\\n\")\n",
        "    s = s.replace(\"\\t\", TAB)\n",
        "    return s\n",
        "\n",
        "def process_tokens(l):\n",
        "    if isinstance(l, str):\n",
        "        l = model.to_str_tokens(l)\n",
        "    elif isinstance(l, torch.Tensor) and len(l.shape)>1:\n",
        "        l = l.squeeze(0)\n",
        "    return [process_token(s) for s in l]\n",
        "\n",
        "def process_tokens_index(l):\n",
        "    if isinstance(l, str):\n",
        "        l = model.to_str_tokens(l)\n",
        "    elif isinstance(l, torch.Tensor) and len(l.shape)>1:\n",
        "        l = l.squeeze(0)\n",
        "    return [f\"{process_token(s)}/{i}\" for i,s in enumerate(l)]\n",
        "\n",
        "def create_vocab_df(logit_vec, make_probs=False, full_vocab=None):\n",
        "    if full_vocab is None:\n",
        "        full_vocab = process_tokens(model.to_str_tokens(torch.arange(model.cfg.d_vocab)))\n",
        "    vocab_df = pd.DataFrame({\"token\": full_vocab, \"logit\": utils.to_numpy(logit_vec)})\n",
        "    if make_probs:\n",
        "        vocab_df[\"log_prob\"] = utils.to_numpy(logit_vec.log_softmax(dim=-1))\n",
        "        vocab_df[\"prob\"] = utils.to_numpy(logit_vec.softmax(dim=-1))\n",
        "    return vocab_df.sort_values(\"logit\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqU0S8NcT-Aj"
      },
      "source": [
        "### Make Token DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ISRtTzfJT_x2"
      },
      "outputs": [],
      "source": [
        "def list_flatten(nested_list):\n",
        "    return [x for y in nested_list for x in y]\n",
        "def make_token_df(tokens, len_prefix=5, len_suffix=1):\n",
        "    str_tokens = [process_tokens(model.to_str_tokens(t)) for t in tokens]\n",
        "    unique_token = [[f\"{s}/{i}\" for i, s in enumerate(str_tok)] for str_tok in str_tokens]\n",
        "\n",
        "    context = []\n",
        "    batch = []\n",
        "    pos = []\n",
        "    label = []\n",
        "    for b in range(tokens.shape[0]):\n",
        "        # context.append([])\n",
        "        # batch.append([])\n",
        "        # pos.append([])\n",
        "        # label.append([])\n",
        "        for p in range(tokens.shape[1]):\n",
        "            prefix = \"\".join(str_tokens[b][max(0, p-len_prefix):p])\n",
        "            if p==tokens.shape[1]-1:\n",
        "                suffix = \"\"\n",
        "            else:\n",
        "                suffix = \"\".join(str_tokens[b][p+1:min(tokens.shape[1]-1, p+1+len_suffix)])\n",
        "            current = str_tokens[b][p]\n",
        "            context.append(f\"{prefix}|{current}|{suffix}\")\n",
        "            batch.append(b)\n",
        "            pos.append(p)\n",
        "            label.append(f\"{b}/{p}\")\n",
        "    # print(len(batch), len(pos), len(context), len(label))\n",
        "    return pd.DataFrame(dict(\n",
        "        str_tokens=list_flatten(str_tokens),\n",
        "        unique_token=list_flatten(unique_token),\n",
        "        context=context,\n",
        "        batch=batch,\n",
        "        pos=pos,\n",
        "        label=label,\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riilpvs-CgoQ"
      },
      "source": [
        "## Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_2LkEWyACgP8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:99: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformer_lens/utils.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file_path, map_location=\"cpu\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gelu-1l into HookedTransformer\n",
            "Changing model dtype to torch.float32\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(DTYPES[cfg[\"enc_dtype\"]])\n",
        "n_layers = model.cfg.n_layers\n",
        "d_model = model.cfg.d_model\n",
        "n_heads = model.cfg.n_heads\n",
        "d_head = model.cfg.d_head\n",
        "d_mlp = model.cfg.d_mlp\n",
        "d_vocab = model.cfg.d_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3D_0qDmBY5K"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2TiJcyRWF3wn"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"NeelNanda/c4-code-20k\", split=\"train\")\n",
        "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
        "tokenized_data = tokenized_data.shuffle(42)\n",
        "all_tokens = tokenized_data[\"tokens\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYrIYDEfBtbL"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JfuCFyCFjGG"
      },
      "source": [
        "## Loading the Autoencoder\n",
        "\n",
        "There are two runs on separate random seeds, along with a bunch of intermediate checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Yyuf42IGFku3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 4096,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.99,\n",
            " 'buffer_batches': 12288,\n",
            " 'buffer_mult': 384,\n",
            " 'buffer_size': 1572864,\n",
            " 'd_mlp': 2048,\n",
            " 'dict_mult': 8,\n",
            " 'enc_dtype': 'fp32',\n",
            " 'l1_coeff': 0.0003,\n",
            " 'lr': 0.0001,\n",
            " 'model_batch_size': 512,\n",
            " 'num_tokens': 2000000000,\n",
            " 'seed': 52,\n",
            " 'seq_len': 128}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformer_lens/utils.py:62: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(file_path, map_location=\"cpu\")\n"
          ]
        }
      ],
      "source": [
        "auto_encoder_run = \"run1\" # @param [\"run1\", \"run2\"]\n",
        "encoder = AutoEncoder.load_from_hf(auto_encoder_run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmgJ6u2hBvLu"
      },
      "source": [
        "## Using the Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP9qP-SiM0Sk"
      },
      "source": [
        "We run the model and replace the MLP activations with those reconstructed from the autoencoder, and get 91% loss recovered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YWlP-98EBygL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss: 3.2546, recons_loss: 3.7507, zero_abl_loss: 8.8403\n",
            "Reconstruction Score: 91.12%\n"
          ]
        }
      ],
      "source": [
        "_ = get_recons_loss(num_batches=5, local_encoder=encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Iok8SZB4fg"
      },
      "source": [
        "## Rare Features Are All The Same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skvpaPClM7Nb"
      },
      "source": [
        "For each feature we can get the frequency at which it's non-zero (per token, averaged across a bunch of batches), and plot a histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5SECGnWLNAlg"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "43eb31a83aff48188b8d1355c5c3a162",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num dead tensor(0., device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "freqs = get_freqs(num_batches = 50, local_encoder = encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Zn7g7WgNTQD"
      },
      "outputs": [],
      "source": [
        "# Add 1e-6.5 so that dead features show up as log_freq -6.5\n",
        "log_freq = (freqs + 10**-6.5).log10()\n",
        "# px.histogram(utils.to_numpy(log_freq), title=\"Log Frequency of Features\", histnorm='percent')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c612eWjYNpdv"
      },
      "source": [
        "We see that it's clearly bimodal! Let's define rare features as those with freq < 1e-4, and look at the cosine sim of each feature with the average rare feature - we see that almost all rare features correspond to this feature!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbM9UZsJN0Uy"
      },
      "outputs": [],
      "source": [
        "is_rare = freqs < 1e-4\n",
        "rare_enc = encoder.W_enc[:, is_rare]\n",
        "rare_mean = rare_enc.mean(-1)\n",
        "# px.histogram(utils.to_numpy(rare_mean @ encoder.W_enc / rare_mean.norm() / encoder.W_enc.norm(dim=0)), title=\"Cosine Sim with Ave Rare Feature\", color=utils.to_numpy(is_rare), labels={\"color\": \"is_rare\", \"count\": \"percent\", \"value\": \"cosine_sim\"}, marginal=\"box\", histnorm=\"percent\", barmode='overlay')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZCqUIB9Bzx_"
      },
      "source": [
        "## Interpreting A Feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m_u8-zXRW2J"
      },
      "source": [
        "Let's go and investigate a non rare feature, feature 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HrsTqVIUKC0"
      },
      "outputs": [],
      "source": [
        "feature_id = 7 # @param {type:\"number\"}\n",
        "batch_size = 128 # @param {type:\"number\"}\n",
        "\n",
        "print(f\"Feature freq: {freqs[7].item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYoGLyl2VpA-"
      },
      "source": [
        "Let's run the model on some text and then use the autoencoder to process the MLP activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBKY9P_PULJs"
      },
      "outputs": [],
      "source": [
        "tokens = all_tokens[:batch_size]\n",
        "_, cache = model.run_with_cache(tokens, stop_at_layer=1, names_filter=utils.get_act_name(\"post\", 0))\n",
        "mlp_acts = cache[utils.get_act_name(\"post\", 0)]\n",
        "mlp_acts_flattened = mlp_acts.reshape(-1, cfg[\"d_mlp\"])\n",
        "loss, x_reconstruct, hidden_acts, l2_loss, l1_loss = encoder(mlp_acts_flattened)\n",
        "# This is equivalent to:\n",
        "# hidden_acts = F.relu((mlp_acts_flattened - encoder.b_dec) @ encoder.W_enc + encoder.b_enc)\n",
        "print(\"hidden_acts.shape\", hidden_acts.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0-vxYAIVtxv"
      },
      "source": [
        "We can now sort and display the top tokens, and we see that this feature activates on text like \" and I\" (ditto for other connectives and pronouns)! It seems interpretable!\n",
        "\n",
        "**Aside:** Note on how to read the context column:\n",
        "\n",
        "A line like \"·himself·as·democratic·socialist·and|·he|·favors\" means that the preceding 5 tokens are \" himself as democratic socialist and\", the current token is \" he\" and the next token is \" favors\".  · are spaces, ↩ is a newline.\n",
        "\n",
        "This gets a bit confusing for this feature, since the pipe separators look a lot like a capital I\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LQet6vaU60H"
      },
      "outputs": [],
      "source": [
        "token_df = make_token_df(tokens)\n",
        "token_df[\"feature\"] = utils.to_numpy(hidden_acts[:, feature_id])\n",
        "token_df.sort_values(\"feature\", ascending=False).head(20).style.background_gradient(\"coolwarm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_SsYbIhWm43"
      },
      "source": [
        "It's easy to misread evidence like the above, so it's useful to take some text and edit it and see how this changes the model's activations. Here's a hacky interactive tool to play around with some text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHWCNBe81qAV"
      },
      "outputs": [],
      "source": [
        "model.cfg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx6MkfEk0osY"
      },
      "outputs": [],
      "source": [
        "s = \"The 1899 Kentucky gubernatorial election was held on November 7, 1899. The Republican incumbent, William Bradley, was term-limited. The Democrats chose William Goebel. Republicans nominated William Taylor. Taylor won by a vote of 193,714 to 191,331. The vote was challenged on grounds of voter fraud, but the Board of Elections, though stocked with pro-Goebel members, certified the result. Democratic legislators began investigations, but before their committee could report, Goebel was shot by an unknown assassin (event pictured) on January 30, 1900. Democrats voided enough votes to swing the election to Goebel, Taylor was deposed, and Goebel was sworn into office on January 31. He died on February 3. The lieutenant governor of Kentucky, J. C. W. Beckham, became governor, and battled Taylor in court. Beckham won on appeal, and Taylor fled to Indiana, fearing arrest as an accomplice. The only persons convicted in connection with the killing were later pardoned; the assassin's identity remains a mystery\"\n",
        "t = model.to_tokens(s)\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbgjmwTnQman"
      },
      "outputs": [],
      "source": [
        "\n",
        "starting_text = \"Hero and I will head to Samantha and Mark's, then he and she will. Then I or you\" # @param {type:\"string\"}\n",
        "make_feature_vis_gradio(feature_id, starting_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjquxRjkQlpq"
      },
      "source": [
        "A final piece of evidence: This is a one layer model, so the neurons can only matter by directly impacting the final logits! We can directly look at how the decoder weights for this feature affect the logits, and see that it boosts `'ll`! This checks out, I and he'll etc is a common construction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUtGmqx8Si-G"
      },
      "outputs": [],
      "source": [
        "logit_effect = encoder.W_dec[feature_id] @ model.W_out[0] @ model.W_U\n",
        "create_vocab_df(logit_effect).head(20).style.background_gradient(\"coolwarm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBb6AwBLSoQU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
